# Amazon Redshift

## Technology
- Column-oriented storage
- Best suited for storing OLAP workloads, summing over a long history
- Internally, it's a modified postgresql

## Architecture
- Massively Parralel Processing(MPP) database **paralelize the execution of one query on multiple CPUs/machines**
- A table is partitioned and partitions are processd in parallel
- Cloud-managed column-oriented, MPP database

![Redshift Arhitecture](exercise-files\images\redshift_architecture.png)

### Leader Node
- Coordinates compute nodes
- Handles external communication
- Optimizes query execution

### Compute nodes
- Each with own CPU, memory and disk
- Scale up: get more powerfull nodes
- Scale out: get more notes

### Node Slices
- Each compute node is logically divided into a number of slices
- A cluster with *n* slices, can process *n* partitions of a table simultaneously

![Redshift Arhitecture](exercise-files\images\redshift_internals.png)

## Configuration examples
- don't forget to setup an Cost Alarm

![Redshift Arhitecture](exercise-files\images\redshift_config_examples.png)

## SQL to SQL ETL

- To copy data from a database to another normally we can do a simple SELECT INTO
- As a more general solution use a ETL server
    - can select on the first database
    - needs a lot fo storage
    - can insert/copy to the destination database
- In AWS
    - we can use an EC2 instance as the ETL Server
    - we can use S3 as storage
    - AWS RDS can write data directly to an S3 bucket
    - Redshift can read direclty from an S3 bucket

## The Big Picture
![Redshift Arhitecture](exercise-files\images\redshift_in_the_big_picture.png)

## Ingesting at Scale: Use COPY
- To transfer data from an S3 staging area to Redshift use the **COPY** command
- Using **INSERT** will be very slow
- If the file is large:
    - It is better to break it up to multiple filkes
    - Ingest in Parrallel
        - Use common prefix
        - or a manifest file
- Other considerations:
    - Better to ingest from the same AWS region
    - Better to compress all the csv files
- (Can have a custom delimiter)
- Redshift suports compression at a column level
- COPY command makes automatic best-effort compression decisions for each column

**Example partition**
```
COPY data FROM 's3://udacity/data/part'
CREDENTIALS '...'
gzip DELIMITER ';' REGION 'us-west-2';
```
Files are like s3://udacity/data/part*.csv.gz

**Example manifest**
```
{
    "etities":[
        {"url":"s3://udacity/data/2020-11-01.csv.gz", "mandatory":true},
        {"url":"s3://udacity/data/2020-11-02.csv.gz", "mandatory":true}
    ]
}
```
```
COPY data
FROM 's3://udacity/data.manifest'
IAM_ROLE '...'
manifest;
```

## ETL from Other Sources
- Can **ingest directly** using ssh from EC2 machine
- S3 needs to be used as a **staging area**
- Usually, an EC2 ETL worker needs to run the ingestion jobs ** orchestrated by a datflow product** like Airflow, Luigi, Nifi, StreamSet or AWS Data Pipeline

## ETL Out of Redshift
- BI app can directly connect to Redshift via JDBC/ODBC
- can run via a command, with write command
```
UNLOAD ('SELECT * FROM table LIMIT 10')
to 's3:/mybucket/table_prefix_'
iam_role '...';
```


