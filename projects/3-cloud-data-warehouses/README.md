# Udacity Data Engineer - Data Warehouse project
by Ovidiu Anicai


## Introduction

I am the Data Engineer at Sparkify and I need to create an ETL pipeline our database from S3 to Redshift, in order for our Analytics team to be able to perform thier tasks.

## Project Datasets

We have two folders with data about songs  `s3://udacity-dend/song_data` and logs `s3://udacity-dend/log_data` and the manifest for the logs  `s3://udacity-dend/log_json_path.json`

## Song Dataset

The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). And it contains a huge list of files organized in subfolders by the track id.
Each file looks like this: 

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

```

## Log Dataset

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. We have them organized by dates and have the following fields: ```artist, auth, firstName, gender, itemSession, lastName, length, level, location, method, page, registration, sessionId, song, status, ts, userAgent, userId```


## Schema for Song Play Analysis

Using the song and event datasets, you'll need to create a star schema optimized for queries on song play analysis. This includes the following tables.

#### Fact Table

1.  songplays - records in event data associated with song plays i.e. records with page `NextSong`
    -   *songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent*

#### Dimension Tables

1.  users - users in the app
    -   *user_id, first_name, last_name, gender, level*
2.  songs - songs in music database
    -   *song_id, title, artist_id, year, duration*
3.  artists - artists in music database
    -   *artist_id, name, location, lattitude, longitude*
4.  time - timestamps of records in songplays broken down into specific units
    -   *start_time, hour, day, week, month, year, weekday*

## Project Steps
1. Design schemas for fact and dimension tables
2. Write the SQL DROP and CREATE statements for all tables
3. Implement the ETL logic for copying the data from S3 to the staging tables on Redshift
4. Implement the ETL logic to load data from staging tables to the analytics tables on Redshift.
5. Start a Redshift cluster
6. Create the tables
7. Run the etl script
8. Test the results with some queries
9. Delete the redshift cluster

## Steps to run

1. Create cluster with ```python cluster_create.py```
2. Create tables using ```python create_tables.py```
3. Run ETL process ```python etl.py```
4. Verify table counts ```python verify_data.py```
5. You can query the database usig the notebook `query_data.ipynb`
6. Delete cluster ```python cluster_delete.py```

## Details
Sparkify needs to be able to run analytical queries on a scalable database with low costs and the setup process needs to be very easy.
As a Data Engineer I implemented an ETL process to ingest all our data and made the new database available to the analytics team via a jupyter notebook.

Starting form our existing files, I imported all data into a staging layer so the compute power for loading the data is done by Redshift via a COPY query. Our analytics team needs a simple star schema for the databases and I transformed the staged tables into the dimension and fact tables they required.
For the queries to perform better I used the things I learned in this class and I used distribution styles and sort keys for some of the tables.
