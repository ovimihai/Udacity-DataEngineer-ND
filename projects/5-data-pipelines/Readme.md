# Udacity Data Engineer - Data pipeline project
by Ovidiu Anicai


## Introduction

I am the Data Engineer at Sparkify and I need to create an ETL pipeline our database from S3 to Redshift, in order for our Analytics team to be able to perform their tasks.

## Project Datasets

We have two folders with data about songs  `s3://udacity-dend/song_data` and logs `s3://udacity-dend/log_data` and the manifest for the logs  `s3://udacity-dend/log_json_path.json`

## Song Dataset

The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). And it contains a huge list of files organized in subfolders by the track id.
Each file looks like this: 

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

```

## Log Dataset

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. We have them organized by dates and have the following fields: ```artist, auth, firstName, gender, itemSession, lastName, length, level, location, method, page, registration, sessionId, song, status, ts, userAgent, userId```


## Schema for Song Play Analysis

Using the song and event datasets, you'll need to create a star schema optimized for queries on song play analysis. This includes the following tables.

#### Fact Table

1.  songplays - records in event data associated with song plays i.e. records with page `NextSong`
    -   *songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent*

#### Dimension Tables

1.  users - users in the app
    -   *user_id, first_name, last_name, gender, level*
2.  songs - songs in music database
    -   *song_id, title, artist_id, year, duration*
3.  artists - artists in music database
    -   *artist_id, name, location, lattitude, longitude*
4.  time - timestamps of records in songplays broken down into specific units
    -   *start_time, hour, day, week, month, year, weekday*

## Project Steps
1. Create a Redshift cluster
2. Create all needed tables
3. Implement all needed parametrized Operators
4. Build an Airflow DAG for the Sparkify music stream logs
5. Run the ETL DAG
6. Check there are no data quality issues
9. Delete the redshift cluster

## Steps to run

1. Go to the run folder `cd run`
1. Create cluster with ```python cluster_create.py```
1. Create tables using ```python create_tables.py```
1. Start the Airflow instance
1. Create the Airflow connection using ```bash create-connections.py```
1. Run the ETL in airflow and check the results
1. Delete cluster ```python cluster_delete.py```

## Details
Sparkify needs to be able to run analytical queries on a scalable database with low costs and the setup process needs to be very easy.
As a Data Engineer I implemented an ETL process to ingest all our data and made the new database available to the analytics team via a jupyter notebook.

As a next step from our existing semi-automatic pipelines we implemented Airflow in our company.
Airflow enables us to schedule data pipelines that import our data from S3 to our Redshift cluster and allows us to put a series of data quality checks so we don't miss the days when our flows don't produce the expected results.
