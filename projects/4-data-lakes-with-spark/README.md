# Udacity Data Engineer - Data Warehouse project
by Ovidiu Anicai


## Introduction

I am the Data Engineer at Sparkify and I need to create an ETL pipeline that takes our data from S3 and processes it with Spark and puts it back in facts-dimensions format, in order for our Analytics team to be able to perform their tasks.

## Project Datasets

We have two folders with data about songs  `s3://udacity-dend/song_data` and logs `s3://udacity-dend/log_data`

## Song Dataset

The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). And it contains a huge list of files organized in subfolders by the track id.
Each file looks like this: 

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

```

## Log Dataset

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. We have them organized by dates and have the following fields: ```artist, auth, firstName, gender, itemSession, lastName, length, level, location, method, page, registration, sessionId, song, status, ts, userAgent, userId```


## Schema for Song Play Analysis

Using the song and event datasets, you'll need to create a star schema optimized for queries on song play analysis. This includes the following tables.

#### Fact Table

1.  songplays - records in event data associated with song plays i.e. records with page `NextSong`
    -   *songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent*

#### Dimension Tables

1.  users - users in the app
    -   *user_id, first_name, last_name, gender, level*
2.  songs - songs in music database
    -   *song_id, title, artist_id, year, duration*
3.  artists - artists in music database
    -   *artist_id, name, location, latitude, longitude*
4.  time - timestamps of records in `songplays` broken down into specific units
    -   *start_time, hour, day, week, month, year, weekday*

## Project Steps
1. Load song_data 
    - partition by year and artist_id
    - save song data to parquet
    - save artists data to parquet
2. Load log_data 
    - filter for song plays
    - build users table and save to parquet
    - build time table (with all date columns) and save to parquet
    - join with song data by artist name, song title and duration
    - save `songplays` table partitioned by year and month

## Steps to run

1. (Optional locally) Extract data archives with `extract.sh`
3. Update `dl.cfg` with the proper `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`
3. Run Spark job `python etl.py`

## Details
Sparkify needs to be able to run analytical queries in an environment that is scalable, has low costs and the setup process needs to be very easy.
As a Data Engineer I implemented an ETL process to be able to process all our data and save it in a format that our analytics team can query easily using Jupyter notebook.

Starting form our existing files, I used spark to load and transform all needed data and then saved it back to S3. Our analytics team needs a simple star schema for the databases and I transformed the staged tables into the dimension and fact tables they required.
